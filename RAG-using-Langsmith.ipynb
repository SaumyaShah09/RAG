{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LangSmith is a tool by LangChain that helps you track, monitor, and debug your AI language model workflows. It records the inputs, outputs, and errors of your models so you can analyze their performance, find issues, and improve your prompts or applications. Think of it as an easy way to understand and optimize how your AI behaves in real time.",
   "id": "d554963dcd34f11d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Smart RAG\n",
    "\n",
    "Loads blog content and splits it into chunks.\n",
    "Tags chunks as beginning, middle, or end.\n",
    "Uses Gemini to:\n",
    "Analyze the question and predict which section to focus on.\n",
    "Retrieve only relevant chunks from that section.\n",
    "Generate a final answer using the filtered content.\n",
    "Itâ€™s smart because it adds context-aware filtering before answering, making responses more accurate."
   ],
   "id": "1e2c72c909d8e7b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loading Env and Initializing Models",
   "id": "18e0f2427026fc2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-04T09:29:01.469154Z",
     "start_time": "2025-06-04T09:28:59.244655Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "assert google_api_key, \"GOOGLE_API_KEY not found in .env\"\n",
    "\n",
    "# LangSmith optional\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "\n",
    "# Gemini LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "\n",
    "# Embedding model\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "\n",
    "# Create an empty vector store (we'll add docs later)\n",
    "vector_store = InMemoryVectorStore(embedding=embeddings)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Setting USER_AGENT and Loading Web Data for RAG",
   "id": "255ae38fe01fea94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:29:09.327652Z",
     "start_time": "2025-06-04T09:29:02.137336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set USER_AGENT\n",
    "os.environ[\"USER_AGENT\"] = \"my-langchain-rag-app/1.0\"\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing import Literal, Annotated\n",
    "from typing_extensions import TypedDict, List\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Add section metadata for smarter filtering\n",
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "# Add documents to vector store\n",
    "_ = vector_store.add_documents(documents=all_splits)\n"
   ],
   "id": "7bb34cfb4090be7e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setting Up Prompt and Defining Application Logic with LangGraph\n",
   "id": "2fefc163773bf9fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:29:13.239432Z",
     "start_time": "2025-06-04T09:29:09.353478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pull RAG prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define structured query schema\n",
    "class Search(TypedDict):\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[Literal[\"beginning\", \"middle\", \"end\"], ..., \"Section to query.\"]\n",
    "\n",
    "# Define state structure\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Step 1: Analyze query and extract structured search info\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "\n",
    "# Step 2: Retrieve documents based on query and section\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"]\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "# Step 3: Generate answer from context\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Build graph with smart steps\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Run test question\n",
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])\n"
   ],
   "id": "207dbf2991cc238a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition involves breaking down complex tasks into smaller, simpler steps. This can be achieved through prompting the model to \"think step by step\" or by using task-specific instructions. It enables efficient handling of complex tasks.\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
