{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-04T12:43:36.372121Z",
     "start_time": "2025-06-04T12:43:36.364137Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#from embedchain.llm.google import GoogleLlm\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "from crewai import Agent, Task, Crew\n",
    "from crewai_tools import SerperDevTool, ScrapeWebsiteTool\n",
    "\n",
    "load_dotenv()\n",
    "from litellm import completion\n",
    "\n",
    "# Use directly in LiteLLM or configure via env vars\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "SERPER_API_KEY = os.getenv(\"SERP_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:43:37.286521Z",
     "start_time": "2025-06-04T12:43:36.400274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Corrected crew_llm instantiation\n",
    "crew_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"google/gemini-1.5-flash\", # Or \"gemini-pro\" etc.\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    max_output_tokens=500, # Parameter name for max tokens\n",
    "    temperature=0.7\n",
    ")"
   ],
   "id": "a90131f92725aae6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Adding the Decision Maker\n",
   "id": "95f235fca2b9aae8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:43:37.306016Z",
     "start_time": "2025-06-04T12:43:37.301140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_local_knowledge(query, context):\n",
    "    \"\"\"Router function to determine if we can answer from local knowledge\"\"\"\n",
    "    prompt = '''Role: Question-Answering Assistant\n",
    "Task: Determine whether the system can answer the user's question based on the provided text.\n",
    "Instructions:\n",
    "    - Analyze the text and identify if it contains the necessary information to answer the user's question.\n",
    "    - Provide a clear and concise response indicating whether the system can answer the question or not.\n",
    "    - Your response should include only a single word. Nothing else, no other text, information, header/footer.\n",
    "Output Format:\n",
    "    - Answer: Yes/No\n",
    "Study the below examples and based on that, respond to the last question.\n",
    "Examples:\n",
    "    Input:\n",
    "        Text: The capital of France is Paris.\n",
    "        User Question: What is the capital of France?\n",
    "    Expected Output:\n",
    "        Answer: Yes\n",
    "    Input:\n",
    "        Text: The population of the United States is over 330 million.\n",
    "        User Question: What is the population of China?\n",
    "    Expected Output:\n",
    "        Answer: No\n",
    "    Input:\n",
    "        User Question: {query}\n",
    "        Text: {text}\n",
    "'''\n",
    "    formatted_prompt = prompt.format(text=context, query=query)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    return response.content.strip().lower() == \"yes\""
   ],
   "id": "c066da770fa8c569",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Web Searching and Scraping Agent\n",
   "id": "924003d954109019"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:43:37.336928Z",
     "start_time": "2025-06-04T12:43:37.327601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup_web_scraping_agent():\n",
    "    \"\"\"Setup the web scraping agent and related components\"\"\"\n",
    "    search_tool = SerperDevTool()  # Tool for performing web searches\n",
    "    scrape_website = ScrapeWebsiteTool()  # Tool for extracting data from websites\n",
    "\n",
    "    # Define the web search agent\n",
    "    web_search_agent = Agent(\n",
    "        role=\"Expert Web Search Agent\",\n",
    "        goal=\"Identify and retrieve relevant web data for user queries\",\n",
    "        backstory=\"An expert in identifying valuable web sources for the user's needs\",\n",
    "        allow_delegation=False,\n",
    "        verbose=True,\n",
    "        llm=crew_llm\n",
    "    )\n",
    "\n",
    "    # Define the web scraping agent\n",
    "    web_scraper_agent = Agent(\n",
    "        role=\"Expert Web Scraper Agent\",\n",
    "        goal=\"Extract and analyze content from specific web pages identified by the search agent\",\n",
    "        backstory=\"A highly skilled web scraper, capable of analyzing and summarizing website content accurately\",\n",
    "        allow_delegation=False,\n",
    "        verbose=True,\n",
    "        llm=crew_llm\n",
    "    )\n",
    "\n",
    "    # Define the web search task\n",
    "    search_task = Task(\n",
    "        description=(\n",
    "            \"Identify the most relevant web page or article for the topic: '{topic}'. \"\n",
    "            \"Use all available tools to search for and provide a link to a web page \"\n",
    "            \"that contains valuable information about the topic. Keep your response concise.\"\n",
    "        ),\n",
    "        expected_output=(\n",
    "            \"A concise summary of the most relevant web page or article for '{topic}', \"\n",
    "            \"including the link to the source and key points from the content.\"\n",
    "        ),\n",
    "        tools=[search_tool],\n",
    "        agent=web_search_agent,\n",
    "    )\n",
    "\n",
    "    # Define the web scraping task\n",
    "    scraping_task = Task(\n",
    "        description=(\n",
    "            \"Extract and analyze data from the given web page or website. Focus on the key sections \"\n",
    "            \"that provide insights into the topic: '{topic}'. Use all available tools to retrieve the content, \"\n",
    "            \"and summarize the key findings in a concise manner.\"\n",
    "        ),\n",
    "        expected_output=(\n",
    "            \"A detailed summary of the content from the given web page or website, highlighting the key insights \"\n",
    "            \"and explaining their relevance to the topic: '{topic}'. Ensure clarity and conciseness.\"\n",
    "        ),\n",
    "        tools=[scrape_website],\n",
    "        agent=web_scraper_agent,\n",
    "    )\n",
    "\n",
    "    # Define the crew to manage agents and tasks\n",
    "    crew = Crew(\n",
    "        agents=[web_search_agent, web_scraper_agent],\n",
    "        tasks=[search_task, scraping_task],\n",
    "        verbose=1,\n",
    "        memory=False,\n",
    "    )\n",
    "    return crew\n",
    "\n",
    "def get_web_content(query):\n",
    "    \"\"\"Get content from web scraping\"\"\"\n",
    "    crew = setup_web_scraping_agent()\n",
    "    result = crew.kickoff(inputs={\"topic\": query})\n",
    "    return result.raw"
   ],
   "id": "1a4d6f910f10cafd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:43:37.372136Z",
     "start_time": "2025-06-04T12:43:37.366808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup_vector_db(pdf_path):\n",
    "    \"\"\"Setup vector database from PDF\"\"\"\n",
    "    # Load and chunk PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Create vector database\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "    vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "    return vector_db\n",
    "\n",
    "def get_local_content(vector_db, query):\n",
    "    \"\"\"Get content from vector database\"\"\"\n",
    "    docs = vector_db.similarity_search(query, k=5)\n",
    "    return \" \".join([doc.page_content for doc in docs])"
   ],
   "id": "fb83d21e95c9878b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:43:37.400371Z",
     "start_time": "2025-06-04T12:43:37.394200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_final_answer(context, query):\n",
    "    \"\"\"Generate final answer using LLM\"\"\"\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Use the provided context to answer the query accurately.\",\n",
    "        ),\n",
    "        (\"system\", f\"Context: {context}\"),\n",
    "        (\"human\", query),\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "def process_query(query, vector_db, local_context):\n",
    "    \"\"\"Main function to process user query\"\"\"\n",
    "    print(f\"Processing query: {query}\")\n",
    "\n",
    "    # Step 1: Check if we can answer from local knowledge\n",
    "    can_answer_locally = check_local_knowledge(query, local_context)\n",
    "    print(f\"Can answer locally: {can_answer_locally}\")\n",
    "\n",
    "    # Step 2: Get context either from local DB or web\n",
    "    if can_answer_locally:\n",
    "        context = get_local_content(vector_db, query)\n",
    "        print(\"Retrieved context from local documents\")\n",
    "    else:\n",
    "        context = get_web_content(query)\n",
    "        print(\"Retrieved context from web scraping\")\n",
    "\n",
    "    # Step 3: Generate final answer\n",
    "    answer = generate_final_answer(context, query)\n",
    "    return answer"
   ],
   "id": "ced635d8e44f9d75",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:44:02.434071Z",
     "start_time": "2025-06-04T12:43:37.427538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Setup\n",
    "    pdf_path = \"genai-principles.pdf\"\n",
    "\n",
    "    # Initialize vector database\n",
    "    print(\"Setting up vector database...\")\n",
    "    vector_db = setup_vector_db(pdf_path)\n",
    "\n",
    "    # Get initial context from PDF for routing\n",
    "    local_context = get_local_content(vector_db, \"\")\n",
    "\n",
    "    # Example usage\n",
    "    query = \"What is Agentic RAG?\"\n",
    "    result = process_query(query, vector_db, local_context)\n",
    "    print(\"\\nFinal Answer:\")\n",
    "    print(result)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "d964b93143542ac8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up vector database...\n",
      "Processing query: What is Agentic RAG?\n",
      "Can answer locally: False\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[36m╭─\u001B[0m\u001B[36m───────────────────────────────────────────\u001B[0m\u001B[36m Crew Execution Started \u001B[0m\u001B[36m────────────────────────────────────────────\u001B[0m\u001B[36m─╮\u001B[0m\n",
       "\u001B[36m│\u001B[0m                                                                                                                 \u001B[36m│\u001B[0m\n",
       "\u001B[36m│\u001B[0m  \u001B[1;36mCrew Execution Started\u001B[0m                                                                                         \u001B[36m│\u001B[0m\n",
       "\u001B[36m│\u001B[0m  \u001B[37mName: \u001B[0m\u001B[36mcrew\u001B[0m                                                                                                     \u001B[36m│\u001B[0m\n",
       "\u001B[36m│\u001B[0m  \u001B[37mID: \u001B[0m\u001B[36m7f2e37fe-2355-451a-be7b-627c4056d17b\u001B[0m                                                                       \u001B[36m│\u001B[0m\n",
       "\u001B[36m│\u001B[0m                                                                                                                 \u001B[36m│\u001B[0m\n",
       "\u001B[36m│\u001B[0m                                                                                                                 \u001B[36m│\u001B[0m\n",
       "\u001B[36m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭──────────────────────────────────────────── Crew Execution Started ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Crew Execution Started</span>                                                                                         <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008080; text-decoration-color: #008080\">crew</span>                                                                                                     <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #008080; text-decoration-color: #008080\">7f2e37fe-2355-451a-be7b-627c4056d17b</span>                                                                       <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">│</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Provider List: https://docs.litellm.ai/docs/providers</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1;95m# Agent:\u001B[0m \u001B[1;92mExpert Web Search Agent\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\"># Agent:</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">Expert Web Search Agent</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[95m## Task:\u001B[0m \u001B[92mIdentify the most relevant web page or article for the topic: 'What is Agentic RAG?'. Use all available \u001B[0m\n",
       "\u001B[92mtools to search for and provide a link to a web page that contains valuable information about the topic. Keep your \u001B[0m\n",
       "\u001B[92mresponse concise.\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">## Task:</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">Identify the most relevant web page or article for the topic: 'What is Agentic RAG?'. Use all available </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">tools to search for and provide a link to a web page that contains valuable information about the topic. Keep your </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">response concise.</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fbeebaf024b41f8831fe123cec345eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Provider List: https://docs.litellm.ai/docs/providers</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[31m╭─\u001B[0m\u001B[31m──────────────────────────────────────────────────\u001B[0m\u001B[31m LLM Error \u001B[0m\u001B[31m──────────────────────────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[1;31m❌ LLM Call Failed\u001B[0m                                                                                             \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[37mError: \u001B[0m\u001B[31mlitellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. \u001B[0m   \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[31mYou passed model=models/google/gemini-1.5-flash\u001B[0m                                                                \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[31m Pass model as E.g. For 'Huggingface' inference endpoints pass in \u001B[0m                                             \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[31m`completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\u001B[0m              \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────────────────────────── LLM Error ───────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">❌ LLM Call Failed</span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Error: </span><span style=\"color: #800000; text-decoration-color: #800000\">litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. </span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #800000; text-decoration-color: #800000\">You passed model=models/google/gemini-1.5-flash</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #800000; text-decoration-color: #800000\"> Pass model as E.g. For 'Huggingface' inference endpoints pass in </span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #800000; text-decoration-color: #800000\">`completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[91m An unknown error occurred. Please check the details below.\u001B[00m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[31m╭─\u001B[0m\u001B[31m────────────────────────────────────────────────\u001B[0m\u001B[31m Task Failure \u001B[0m\u001B[31m─────────────────────────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[1;31mTask Failed\u001B[0m                                                                                                    \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[37mName: \u001B[0m\u001B[31m51e58d3c-530e-4c1b-94e7-69e6d0e6f32c\u001B[0m                                                                     \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[37mAgent: \u001B[0m\u001B[31mExpert Web Search Agent\u001B[0m                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭───────────────────────────────────────────────── Task Failure ──────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Task Failed</span>                                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #800000; text-decoration-color: #800000\">51e58d3c-530e-4c1b-94e7-69e6d0e6f32c</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #800000; text-decoration-color: #800000\">Expert Web Search Agent</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[31m╭─\u001B[0m\u001B[31m────────────────────────────────────────────────\u001B[0m\u001B[31m Crew Failure \u001B[0m\u001B[31m─────────────────────────────────────────────────\u001B[0m\u001B[31m─╮\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[1;31mCrew Execution Failed\u001B[0m                                                                                          \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[37mName: \u001B[0m\u001B[31mcrew\u001B[0m                                                                                                     \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m  \u001B[37mID: \u001B[0m\u001B[31m7f2e37fe-2355-451a-be7b-627c4056d17b\u001B[0m                                                                       \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m│\u001B[0m                                                                                                                 \u001B[31m│\u001B[0m\n",
       "\u001B[31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭───────────────────────────────────────────────── Crew Failure ──────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Crew Execution Failed</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #800000; text-decoration-color: #800000\">crew</span>                                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #800000; text-decoration-color: #800000\">7f2e37fe-2355-451a-be7b-627c4056d17b</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=models/google/gemini-1.5-flash\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mBadRequestError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m     \u001B[38;5;28mprint\u001B[39m(result)\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 14\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[32m     13\u001B[39m query = \u001B[33m\"\u001B[39m\u001B[33mWhat is Agentic RAG?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m result = \u001B[43mprocess_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvector_db\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_context\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mFinal Answer:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     16\u001B[39m \u001B[38;5;28mprint\u001B[39m(result)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 27\u001B[39m, in \u001B[36mprocess_query\u001B[39m\u001B[34m(query, vector_db, local_context)\u001B[39m\n\u001B[32m     25\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mRetrieved context from local documents\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m     context = \u001B[43mget_web_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mRetrieved context from web scraping\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# Step 3: Generate final answer\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 68\u001B[39m, in \u001B[36mget_web_content\u001B[39m\u001B[34m(query)\u001B[39m\n\u001B[32m     66\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Get content from web scraping\"\"\"\u001B[39;00m\n\u001B[32m     67\u001B[39m crew = setup_web_scraping_agent()\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m result = \u001B[43mcrew\u001B[49m\u001B[43m.\u001B[49m\u001B[43mkickoff\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtopic\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result.raw\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\crew.py:661\u001B[39m, in \u001B[36mCrew.kickoff\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    658\u001B[39m metrics: List[UsageMetrics] = []\n\u001B[32m    660\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.process == Process.sequential:\n\u001B[32m--> \u001B[39m\u001B[32m661\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_sequential_process\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    662\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.process == Process.hierarchical:\n\u001B[32m    663\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._run_hierarchical_process()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\crew.py:773\u001B[39m, in \u001B[36mCrew._run_sequential_process\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    771\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34m_run_sequential_process\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> CrewOutput:\n\u001B[32m    772\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_execute_tasks\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\crew.py:876\u001B[39m, in \u001B[36mCrew._execute_tasks\u001B[39m\u001B[34m(self, tasks, start_index, was_replayed)\u001B[39m\n\u001B[32m    873\u001B[39m     futures.clear()\n\u001B[32m    875\u001B[39m context = \u001B[38;5;28mself\u001B[39m._get_context(task, task_outputs)\n\u001B[32m--> \u001B[39m\u001B[32m876\u001B[39m task_output = \u001B[43mtask\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute_sync\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m    \u001B[49m\u001B[43magent\u001B[49m\u001B[43m=\u001B[49m\u001B[43magent_to_use\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mList\u001B[49m\u001B[43m[\u001B[49m\u001B[43mBaseTool\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools_for_task\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    880\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    881\u001B[39m task_outputs.append(task_output)\n\u001B[32m    882\u001B[39m \u001B[38;5;28mself\u001B[39m._process_task_result(task, task_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\task.py:351\u001B[39m, in \u001B[36mTask.execute_sync\u001B[39m\u001B[34m(self, agent, context, tools)\u001B[39m\n\u001B[32m    344\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34mexecute_sync\u001B[39m(\n\u001B[32m    345\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    346\u001B[39m     agent: Optional[BaseAgent] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    347\u001B[39m     context: Optional[\u001B[38;5;28mstr\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    348\u001B[39m     tools: Optional[List[BaseTool]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    349\u001B[39m ) -> TaskOutput:\n\u001B[32m    350\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Execute the task synchronously.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m351\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_execute_core\u001B[49m\u001B[43m(\u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\task.py:495\u001B[39m, in \u001B[36mTask._execute_core\u001B[39m\u001B[34m(self, agent, context, tools)\u001B[39m\n\u001B[32m    493\u001B[39m \u001B[38;5;28mself\u001B[39m.end_time = datetime.datetime.now()\n\u001B[32m    494\u001B[39m crewai_event_bus.emit(\u001B[38;5;28mself\u001B[39m, TaskFailedEvent(error=\u001B[38;5;28mstr\u001B[39m(e), task=\u001B[38;5;28mself\u001B[39m))\n\u001B[32m--> \u001B[39m\u001B[32m495\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\task.py:415\u001B[39m, in \u001B[36mTask._execute_core\u001B[39m\u001B[34m(self, agent, context, tools)\u001B[39m\n\u001B[32m    413\u001B[39m \u001B[38;5;28mself\u001B[39m.processed_by_agents.add(agent.role)\n\u001B[32m    414\u001B[39m crewai_event_bus.emit(\u001B[38;5;28mself\u001B[39m, TaskStartedEvent(context=context, task=\u001B[38;5;28mself\u001B[39m))\n\u001B[32m--> \u001B[39m\u001B[32m415\u001B[39m result = \u001B[43magent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute_task\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    416\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    417\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    418\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    419\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    421\u001B[39m pydantic_output, json_output = \u001B[38;5;28mself\u001B[39m._export_output(result)\n\u001B[32m    422\u001B[39m task_output = TaskOutput(\n\u001B[32m    423\u001B[39m     name=\u001B[38;5;28mself\u001B[39m.name,\n\u001B[32m    424\u001B[39m     description=\u001B[38;5;28mself\u001B[39m.description,\n\u001B[32m   (...)\u001B[39m\u001B[32m    430\u001B[39m     output_format=\u001B[38;5;28mself\u001B[39m._get_output_format(),\n\u001B[32m    431\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\agent.py:420\u001B[39m, in \u001B[36mAgent.execute_task\u001B[39m\u001B[34m(self, task, context, tools)\u001B[39m\n\u001B[32m    410\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m e.\u001B[34m__class__\u001B[39m.\u001B[34m__module__\u001B[39m.startswith(\u001B[33m\"\u001B[39m\u001B[33mlitellm\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    411\u001B[39m     \u001B[38;5;66;03m# Do not retry on litellm errors\u001B[39;00m\n\u001B[32m    412\u001B[39m     crewai_event_bus.emit(\n\u001B[32m    413\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    414\u001B[39m         event=AgentExecutionErrorEvent(\n\u001B[32m   (...)\u001B[39m\u001B[32m    418\u001B[39m         ),\n\u001B[32m    419\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m420\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m    421\u001B[39m \u001B[38;5;28mself\u001B[39m._times_executed += \u001B[32m1\u001B[39m\n\u001B[32m    422\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._times_executed > \u001B[38;5;28mself\u001B[39m.max_retry_limit:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\agent.py:396\u001B[39m, in \u001B[36mAgent.execute_task\u001B[39m\u001B[34m(self, task, context, tools)\u001B[39m\n\u001B[32m    392\u001B[39m         result = \u001B[38;5;28mself\u001B[39m._execute_with_timeout(\n\u001B[32m    393\u001B[39m             task_prompt, task, \u001B[38;5;28mself\u001B[39m.max_execution_time\n\u001B[32m    394\u001B[39m         )\n\u001B[32m    395\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m396\u001B[39m         result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_execute_without_timeout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask_prompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    398\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    399\u001B[39m     \u001B[38;5;66;03m# Propagate TimeoutError without retry\u001B[39;00m\n\u001B[32m    400\u001B[39m     crewai_event_bus.emit(\n\u001B[32m    401\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    402\u001B[39m         event=AgentExecutionErrorEvent(\n\u001B[32m   (...)\u001B[39m\u001B[32m    406\u001B[39m         ),\n\u001B[32m    407\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\agent.py:492\u001B[39m, in \u001B[36mAgent._execute_without_timeout\u001B[39m\u001B[34m(self, task_prompt, task)\u001B[39m\n\u001B[32m    482\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34m_execute_without_timeout\u001B[39m(\u001B[38;5;28mself\u001B[39m, task_prompt: \u001B[38;5;28mstr\u001B[39m, task: Task) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    483\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Execute a task without a timeout.\u001B[39;00m\n\u001B[32m    484\u001B[39m \n\u001B[32m    485\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    490\u001B[39m \u001B[33;03m        The output of the agent.\u001B[39;00m\n\u001B[32m    491\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m492\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43magent_executor\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    493\u001B[39m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    494\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask_prompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    495\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtool_names\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43magent_executor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtools_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    496\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtools\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43magent_executor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtools_description\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mask_for_human_input\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhuman_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    498\u001B[39m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\n\u001B[32m    499\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m[\u001B[33m\"\u001B[39m\u001B[33moutput\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:121\u001B[39m, in \u001B[36mCrewAgentExecutor.invoke\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    118\u001B[39m handle_unknown_error(\u001B[38;5;28mself\u001B[39m._printer, e)\n\u001B[32m    119\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m e.\u001B[34m__class__\u001B[39m.\u001B[34m__module__\u001B[39m.startswith(\u001B[33m\"\u001B[39m\u001B[33mlitellm\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# Do not retry on litellm errors\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m    122\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    123\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:110\u001B[39m, in \u001B[36mCrewAgentExecutor.invoke\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    107\u001B[39m \u001B[38;5;28mself\u001B[39m.ask_for_human_input = \u001B[38;5;28mbool\u001B[39m(inputs.get(\u001B[33m\"\u001B[39m\u001B[33mask_for_human_input\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[32m    109\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m110\u001B[39m     formatted_answer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_invoke_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m:\n\u001B[32m    112\u001B[39m     \u001B[38;5;28mself\u001B[39m._printer.print(\n\u001B[32m    113\u001B[39m         content=\u001B[33m\"\u001B[39m\u001B[33mAgent failed to reach a final answer. This is likely a bug - please report it.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    114\u001B[39m         color=\u001B[33m\"\u001B[39m\u001B[33mred\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    115\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:206\u001B[39m, in \u001B[36mCrewAgentExecutor._invoke_loop\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    203\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    204\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m e.\u001B[34m__class__\u001B[39m.\u001B[34m__module__\u001B[39m.startswith(\u001B[33m\"\u001B[39m\u001B[33mlitellm\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    205\u001B[39m         \u001B[38;5;66;03m# Do not retry on litellm errors\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m    207\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_context_length_exceeded(e):\n\u001B[32m    208\u001B[39m         handle_context_length(\n\u001B[32m    209\u001B[39m             respect_context_window=\u001B[38;5;28mself\u001B[39m.respect_context_window,\n\u001B[32m    210\u001B[39m             printer=\u001B[38;5;28mself\u001B[39m._printer,\n\u001B[32m   (...)\u001B[39m\u001B[32m    214\u001B[39m             i18n=\u001B[38;5;28mself\u001B[39m._i18n,\n\u001B[32m    215\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:153\u001B[39m, in \u001B[36mCrewAgentExecutor._invoke_loop\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    142\u001B[39m     formatted_answer = handle_max_iterations_exceeded(\n\u001B[32m    143\u001B[39m         formatted_answer,\n\u001B[32m    144\u001B[39m         printer=\u001B[38;5;28mself\u001B[39m._printer,\n\u001B[32m   (...)\u001B[39m\u001B[32m    148\u001B[39m         callbacks=\u001B[38;5;28mself\u001B[39m.callbacks,\n\u001B[32m    149\u001B[39m     )\n\u001B[32m    151\u001B[39m enforce_rpm_limit(\u001B[38;5;28mself\u001B[39m.request_within_rpm_limit)\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m answer = \u001B[43mget_llm_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    154\u001B[39m \u001B[43m    \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    155\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    156\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    157\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprinter\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_printer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    158\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    159\u001B[39m formatted_answer = process_llm_response(answer, \u001B[38;5;28mself\u001B[39m.use_stop_words)\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(formatted_answer, AgentAction):\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# Extract agent fingerprint if available\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\utilities\\agent_utils.py:157\u001B[39m, in \u001B[36mget_llm_response\u001B[39m\u001B[34m(llm, messages, callbacks, printer)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    153\u001B[39m     printer.print(\n\u001B[32m    154\u001B[39m         content=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError during LLM call: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    155\u001B[39m         color=\u001B[33m\"\u001B[39m\u001B[33mred\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    156\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m157\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m    158\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m answer:\n\u001B[32m    159\u001B[39m     printer.print(\n\u001B[32m    160\u001B[39m         content=\u001B[33m\"\u001B[39m\u001B[33mReceived None or empty response from LLM call.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    161\u001B[39m         color=\u001B[33m\"\u001B[39m\u001B[33mred\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    162\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\utilities\\agent_utils.py:148\u001B[39m, in \u001B[36mget_llm_response\u001B[39m\u001B[34m(llm, messages, callbacks, printer)\u001B[39m\n\u001B[32m    146\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001B[39;00m\n\u001B[32m    147\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m148\u001B[39m     answer = \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    149\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    150\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    151\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    152\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    153\u001B[39m     printer.print(\n\u001B[32m    154\u001B[39m         content=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError during LLM call: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    155\u001B[39m         color=\u001B[33m\"\u001B[39m\u001B[33mred\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    156\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\llm.py:924\u001B[39m, in \u001B[36mLLM.call\u001B[39m\u001B[34m(self, messages, tools, callbacks, available_functions)\u001B[39m\n\u001B[32m    920\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._handle_streaming_response(\n\u001B[32m    921\u001B[39m             params, callbacks, available_functions\n\u001B[32m    922\u001B[39m         )\n\u001B[32m    923\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m924\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_handle_non_streaming_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    925\u001B[39m \u001B[43m            \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mavailable_functions\u001B[49m\n\u001B[32m    926\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m LLMContextLengthExceededException:\n\u001B[32m    929\u001B[39m     \u001B[38;5;66;03m# Re-raise LLMContextLengthExceededException as it should be handled\u001B[39;00m\n\u001B[32m    930\u001B[39m     \u001B[38;5;66;03m# by the CrewAgentExecutor._invoke_loop method, which can then decide\u001B[39;00m\n\u001B[32m    931\u001B[39m     \u001B[38;5;66;03m# whether to summarize the content or abort based on the respect_context_window flag\u001B[39;00m\n\u001B[32m    932\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\crewai\\llm.py:763\u001B[39m, in \u001B[36mLLM._handle_non_streaming_response\u001B[39m\u001B[34m(self, params, callbacks, available_functions)\u001B[39m\n\u001B[32m    757\u001B[39m \u001B[38;5;66;03m# --- 1) Make the completion call\u001B[39;00m\n\u001B[32m    758\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    759\u001B[39m     \u001B[38;5;66;03m# Attempt to make the completion call, but catch context window errors\u001B[39;00m\n\u001B[32m    760\u001B[39m     \u001B[38;5;66;03m# and convert them to our own exception type for consistent handling\u001B[39;00m\n\u001B[32m    761\u001B[39m     \u001B[38;5;66;03m# across the codebase. This allows CrewAgentExecutor to handle context\u001B[39;00m\n\u001B[32m    762\u001B[39m     \u001B[38;5;66;03m# length issues appropriately.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m763\u001B[39m     response = \u001B[43mlitellm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletion\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    764\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ContextWindowExceededError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    765\u001B[39m     \u001B[38;5;66;03m# Convert litellm's context window error to our own exception type\u001B[39;00m\n\u001B[32m    766\u001B[39m     \u001B[38;5;66;03m# for consistent handling in the rest of the codebase\u001B[39;00m\n\u001B[32m    767\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m LLMContextLengthExceededException(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\litellm\\utils.py:1255\u001B[39m, in \u001B[36mclient.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1251\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m logging_obj:\n\u001B[32m   1252\u001B[39m     logging_obj.failure_handler(\n\u001B[32m   1253\u001B[39m         e, traceback_exception, start_time, end_time\n\u001B[32m   1254\u001B[39m     )  \u001B[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1255\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\litellm\\utils.py:1133\u001B[39m, in \u001B[36mclient.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1131\u001B[39m         print_verbose(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError while checking max token limit: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1132\u001B[39m \u001B[38;5;66;03m# MODEL CALL\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1133\u001B[39m result = \u001B[43moriginal_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1134\u001B[39m end_time = datetime.datetime.now()\n\u001B[32m   1135\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m kwargs[\u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\litellm\\main.py:3216\u001B[39m, in \u001B[36mcompletion\u001B[39m\u001B[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001B[39m\n\u001B[32m   3213\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n\u001B[32m   3214\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   3215\u001B[39m     \u001B[38;5;66;03m## Map to OpenAI Exception\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3216\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exception_type(\n\u001B[32m   3217\u001B[39m         model=model,\n\u001B[32m   3218\u001B[39m         custom_llm_provider=custom_llm_provider,\n\u001B[32m   3219\u001B[39m         original_exception=e,\n\u001B[32m   3220\u001B[39m         completion_kwargs=args,\n\u001B[32m   3221\u001B[39m         extra_kwargs=kwargs,\n\u001B[32m   3222\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\litellm\\main.py:1031\u001B[39m, in \u001B[36mcompletion\u001B[39m\u001B[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001B[39m\n\u001B[32m   1029\u001B[39m     model = deployment_id\n\u001B[32m   1030\u001B[39m     custom_llm_provider = \u001B[33m\"\u001B[39m\u001B[33mazure\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1031\u001B[39m model, custom_llm_provider, dynamic_api_key, api_base = \u001B[43mget_llm_provider\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1032\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1033\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcustom_llm_provider\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1034\u001B[39m \u001B[43m    \u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m=\u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1035\u001B[39m \u001B[43m    \u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m=\u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1036\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1038\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1039\u001B[39m     provider_specific_header \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1040\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m provider_specific_header[\u001B[33m\"\u001B[39m\u001B[33mcustom_llm_provider\u001B[39m\u001B[33m\"\u001B[39m] == custom_llm_provider\n\u001B[32m   1041\u001B[39m ):\n\u001B[32m   1042\u001B[39m     headers.update(provider_specific_header[\u001B[33m\"\u001B[39m\u001B[33mextra_headers\u001B[39m\u001B[33m\"\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:360\u001B[39m, in \u001B[36mget_llm_provider\u001B[39m\u001B[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001B[39m\n\u001B[32m    358\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    359\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, litellm.exceptions.BadRequestError):\n\u001B[32m--> \u001B[39m\u001B[32m360\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m    361\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    362\u001B[39m         error_str = (\n\u001B[32m    363\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mGetLLMProvider Exception - \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33moriginal model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    364\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\venv_master\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:337\u001B[39m, in \u001B[36mget_llm_provider\u001B[39m\u001B[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001B[39m\n\u001B[32m    335\u001B[39m     error_str = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m Pass model as E.g. For \u001B[39m\u001B[33m'\u001B[39m\u001B[33mHuggingface\u001B[39m\u001B[33m'\u001B[39m\u001B[33m inference endpoints pass in `completion(model=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mhuggingface/starcoder\u001B[39m\u001B[33m'\u001B[39m\u001B[33m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    336\u001B[39m     \u001B[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m337\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m litellm.exceptions.BadRequestError(  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[32m    338\u001B[39m         message=error_str,\n\u001B[32m    339\u001B[39m         model=model,\n\u001B[32m    340\u001B[39m         response=httpx.Response(\n\u001B[32m    341\u001B[39m             status_code=\u001B[32m400\u001B[39m,\n\u001B[32m    342\u001B[39m             content=error_str,\n\u001B[32m    343\u001B[39m             request=httpx.Request(method=\u001B[33m\"\u001B[39m\u001B[33mcompletion\u001B[39m\u001B[33m\"\u001B[39m, url=\u001B[33m\"\u001B[39m\u001B[33mhttps://github.com/BerriAI/litellm\u001B[39m\u001B[33m\"\u001B[39m),  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[32m    344\u001B[39m         ),\n\u001B[32m    345\u001B[39m         llm_provider=\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    346\u001B[39m     )\n\u001B[32m    347\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m api_base \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(api_base, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    348\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[32m    349\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mapi base needs to be a string. api_base=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(api_base)\n\u001B[32m    350\u001B[39m     )\n",
      "\u001B[31mBadRequestError\u001B[39m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=models/google/gemini-1.5-flash\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
